dyad_census(net)
betweenCent <- betweenness(net)
library(igraph)
betweenCent <- betweenness(net)
cor(betweenCent,eigenCent)
eigenCent <- eigen_centrality(net)$vector
# let's plot the score on the graph:
bins <- unique(quantile(eigenCent, seq(0,1,length.out=15)))
vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)
my_col = heat.colors(length(bins))
colorVals <- rev(my_col)[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# so who are the more relevant user?
sort(eigenCent,decreasing=TRUE)[1:10]
library(igraph)
net <- graph_from_data_frame(d=mention_counts, vertices = usernames, directed=T)
net <- simplify(net, remove.loops=T)
V(net)$size <- 5
l <- layout.fruchterman.reingold(net)
E(net)$width <- E(net)$weight/50
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
deg <- degree(net, mode="in") # Node degree -> most mentioned nodes on the social
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
hist(deg, breaks=1:vcount(net)-1, main="Histogram of Node Degree")
deg.dist <- degree_distribution(net, cumulative=T, mode="in")
plot( x=0:max(deg), y=1-deg.dist, pch=19, cex=1.2, col="orange", xlab="Degree", ylab="Cumulative Frequency")
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
deg <- degree(net, mode="out") # Node degree -> most active nodes on the social
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
hist(deg, breaks=1:vcount(net)-1, main="Histogram of Node Degree")
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
reciprocity(net)
dyad_census(net)
net <- net - V(net)[degree(net, mode="all")==0]
l <- layout.fruchterman.reingold(net)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# we will at first measure eigen centrality, that is a refinement that assigns higher weight to vertex for being connected to vertices which are themselves important
eigenCent <- eigen_centrality(net)$vector
# let's plot the score on the graph:
bins <- unique(quantile(eigenCent, seq(0,1,length.out=15)))
vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)
my_col = heat.colors(length(bins))
colorVals <- rev(my_col)[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# so who are the more relevant user?
sort(eigenCent,decreasing=TRUE)[1:10]
l <- layout_on_sphere(net.bg)
l <- layout_on_sphere(net)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
l <- layout_on_sphere(net)
E(net)$width <- 1
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
l <- layout_on_sphere(net)
E(net)$width <- 1
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
E(net)$width <- .5
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
E(net)$width <- .5
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
l <- layout.fruchterman.reingold(net)
betweenCent <- betweenness(net)
cor(betweenCent,eigenCent)
bins <- unique(quantile(betweenCent, seq(0,1,length.out=30)))
vals <- cut(betweenCent, bins, labels=FALSE, include.lowest=TRUE)
colorVals <- rev(heat.colors(length(bins)))[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# Which are the most useful users to the graph connection?
sort(betweenCent,decreasing=TRUE)[1:10]
eigenCent <- eigen_centrality(net)$vector
# let's plot the score on the graph:
bins <- unique(quantile(eigenCent, seq(0,1,length.out=15)))
vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)
my_col = heat.colors(length(bins))
colorVals <- rev(my_col)[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# so who are the more relevant user?
sort(eigenCent,decreasing=TRUE)[1:10]
betweenCent <- betweenness(net)
cor(betweenCent,eigenCent)
bins <- unique(quantile(betweenCent, seq(0,1,length.out=30)))
vals <- cut(betweenCent, bins, labels=FALSE, include.lowest=TRUE)
colorVals <- rev(heat.colors(length(bins)))[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# Which are the most useful users to the graph connection?
sort(betweenCent,decreasing=TRUE)[1:10]
# we will now measure betweenness centrality, to see which are the vertices that connect important parts of the graph
betweenCent <- betweenness(net)
cor(betweenCent,eigenCent)
bins <- unique(quantile(betweenCent, seq(0,1,length.out=30)))
vals <- cut(betweenCent, bins, labels=FALSE, include.lowest=TRUE)
colorVals <- rev(heat.colors(length(bins)))[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# Which are the most useful users to the graph connection?
sort(betweenCent,decreasing=TRUE)[1:10]
# we will now measure betweenness centrality, to see which are the vertices that connect important parts of the graph
betweenCent <- betweenness(net)
cor(betweenCent,eigenCent)
bins <- unique(quantile(betweenCent, seq(0,1,length.out=30)))
vals <- cut(betweenCent, bins, labels=FALSE, include.lowest=TRUE)
colorVals <- rev(heat.colors(length(bins)))[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# Which are the most useful users to the graph connection?
sort(betweenCent,decreasing=TRUE)[1:10]
eigenCent <- eigen_centrality(net)$vector
# let's plot the score on the graph:
bins <- unique(quantile(eigenCent, seq(0,1,length.out=15)))
vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)
my_col = heat.colors(length(bins))
colorVals <- rev(my_col)[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# so who are the more relevant user?
sort(eigenCent,decreasing=TRUE)[1:10]
head(tweet)
library(quanteda)
corpus = corpus(tweet, text_field = "tweets")
summary(corpus)
df <- read.csv("tweets.csv") # import the data
head(df)
tweets <- c(df$username, df$tweets)
head(tweets)
tweets <- df[, c("username", "tweets")]
head(tweets)
corpus = corpus(tweets, text_field = "tweets")
summary(corpus)
tweets <- df[, c("username", "tweets")]
tweets$tweets <- gsub("ENGLISH TRANSLATION:", "", tweets$tweets, ignore.case = TRUE)
head(tweets)
tweets <- df[, c("username", "tweets")]
#tweets$tweets <- gsub("ENGLISH TRANSLATION:", "", tweets$tweets, ignore.case = TRUE)
head(tweets)
tweets <- df[, c("username", "tweets")]
tweets$tweets <- gsub("ENGLISH TRANSLATION:", "", tweets$tweets, ignore.case = TRUE)
head(tweets)
corpus = corpus(tweets, text_field = "tweets")
summary(corpus)
tweets <- df[, c("username", "tweets")]
tweets$tweets <- gsub("ENGLISH TRANSLATION:", "", tweets$tweets, ignore.case = TRUE)
tweets$tweets <- gsub("http[^[:space:]]+", "", tweets$tweets)         # remove URLs
tweets$tweets <- gsub("@\\w+", "", tweets$tweets)                     # remove mentions
tweets$tweets <- gsub("#\\w+", "", tweets$tweets)                     # remove hashtags
head(tweets)
tweets <- df[, c("username", "tweets")]
# we are interested only on the message in the tweet, not to link or mentions
tweets$tweets <- gsub("ENGLISH TRANSLATION:", "", tweets$tweets, ignore.case = TRUE)
tweets$tweets <- gsub("http[^[:space:]]+", "", tweets$tweets)
tweets$tweets <- gsub("@\\w+", "", tweets$tweets)
head(tweets)
corpus = corpus(tweets, text_field = "tweets")
summary(corpus)
doc.tokens = tokens(corpus) # tokenize the text (split each document into individual tokens)
doc.tokens = tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE) # we remove punctuation and numbers from the token (they are noise)
doc.tokens = tokens_select(doc.tokens, stopwords(language = "en", source = "snowball", simplify = TRUE), selection ='remove') # we remove common english words like "is", "the", "and"
doc.tokens = tokens_tolower(doc.tokens) # convert all words to lower case, making analysis consistent
toks_ngram = tokens_ngrams(doc.tokens, n = 1:2) # non only single words but also pairs of consecutive words, we now have a richer set of features than just individual words
toks_ngram
toks_ngram = tokens_ngrams(doc.tokens, n = 2) # non only single words but also pairs of consecutive words, we now have a richer set of features than just individual words
toks_ngram
dfmat = dfm(toks_ngram) %>% dfm_trim(min_termfreq = 10) # this create a DFM (rows are the documents, columns are the features -> cells contains frequency of feature in a document) and we filter out rare terms (we only want terms with frequency > 10)
dfmat
library(quanteda.textstats)
features_dfm = textstat_frequency(dfmat1, n = 100)
library(quanteda.textstats)
features_dfm = textstat_frequency(dfmat, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
tweets <- df[, c("username", "tweets")]
# we are interested only on the message in the tweet, not to link or mentions
tweets$tweets <- gsub("ENGLISH TRANSLATION:", "", tweets$tweets, ignore.case = TRUE)
tweets$tweets <- gsub("http[^[:space:]]+", "", tweets$tweets)
tweets$tweets <- gsub("@\\w+", "", tweets$tweets)
tweets$tweets <- gsub("#\\w+", "", tweets$tweets)
corpus = corpus(tweets, text_field = "tweets")
summary(corpus)
doc.tokens = tokens(corpus) # tokenize the text (split each document into individual tokens)
doc.tokens = tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE) # we remove punctuation and numbers from the token (they are noise)
doc.tokens = tokens_select(doc.tokens, stopwords(language = "en", source = "snowball", simplify = TRUE), selection ='remove') # we remove common english words like "is", "the", "and"
doc.tokens = tokens_tolower(doc.tokens) # convert all words to lower case, making analysis consistent
toks_ngram = tokens_ngrams(doc.tokens, n = 2) # non only single words but also pairs of consecutive words, we now have a richer set of features than just individual words
toks_ngram
dfmat = dfm(toks_ngram) %>% dfm_trim(min_termfreq = 10) # this create a DFM (rows are the documents, columns are the features -> cells contains frequency of feature in a document) and we filter out rare terms (we only want terms with frequency > 10)
dfmat
library(quanteda.textstats)
features_dfm = textstat_frequency(dfmat, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
doc.tokens = tokens(corpus) # tokenize the text (split each document into individual tokens)
doc.tokens = tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE) # we remove punctuation and numbers from the token (they are noise)
doc.tokens = tokens_select(doc.tokens, stopwords(language = "en", source = "snowball", simplify = TRUE), selection ='remove') # we remove common english words like "is", "the", "and"
doc.tokens = tokens_tolower(doc.tokens) # convert all words to lower case, making analysis consistent
doc.tokens <- tokens_keep(doc.tokens, pattern = "^[a-z]+$", valuetype = "regex") # we keep only tokens that are entirely lowercase alphabet characters
toks_ngram = tokens_ngrams(doc.tokens, n = 2) # non only single words but also pairs of consecutive words, we now have a richer set of features than just individual words
toks_ngram
dfmat = dfm(toks_ngram) %>% dfm_trim(min_termfreq = 10) # this create a DFM (rows are the documents, columns are the features -> cells contains frequency of feature in a document) and we filter out rare terms (we only want terms with frequency > 10)
dfmat
library(quanteda.textstats)
features_dfm = textstat_frequency(dfmat, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
# we are interested only on the message in the tweet, not to link or mentions
tweets$tweets <- gsub("ENGLISH TRANSLATION:|rt", "", tweets$tweets, ignore.case = TRUE)
tweets$tweets <- gsub("http[^[:space:]]+", "", tweets$tweets)
tweets$tweets <- gsub("@\\w+", "", tweets$tweets)
tweets$tweets <- gsub("#\\w+", "", tweets$tweets)
head(tweets)
corpus = corpus(tweets, text_field = "tweets")
summary(corpus)
doc.tokens = tokens(corpus) # tokenize the text (split each document into individual tokens)
doc.tokens = tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE) # we remove punctuation and numbers from the token (they are noise)
doc.tokens = tokens_select(doc.tokens, stopwords(language = "en", source = "snowball", simplify = TRUE), selection ='remove') # we remove common english words like "is", "the", "and"
doc.tokens = tokens_tolower(doc.tokens) # convert all words to lower case, making analysis consistent
doc.tokens <- tokens_keep(doc.tokens, pattern = "^[a-z]+$", valuetype = "regex") # we keep only tokens that are entirely lowercase alphabet characters
toks_ngram = tokens_ngrams(doc.tokens, n = 2) # non only single words but also pairs of consecutive words, we now have a richer set of features than just individual words
toks_ngram
dfmat = dfm(toks_ngram) %>% dfm_trim(min_termfreq = 10) # this create a DFM (rows are the documents, columns are the features -> cells contains frequency of feature in a document) and we filter out rare terms (we only want terms with frequency > 10)
dfmat
library(quanteda.textstats)
features_dfm = textstat_frequency(dfmat, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
library(quanteda.textplots)
textplot_wordcloud(dfmat)
dfmat = dfm(toks_ngram) %>% dfm_trim(min_termfreq = 20) # this create a DFM (rows are the documents, columns are the features -> cells contains frequency of feature in a document) and we filter out rare terms (we only want terms with frequency > 10)
dfmat
library(quanteda.textstats)
features_dfm = textstat_frequency(dfmat, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
library(quanteda.textplots)
textplot_wordcloud(dfmat)
hashtags <- df[, c("username", "tweets")]
# this time we are interested only in the hashtags
hashtags$tweets <- regmatches(hashtags$tweets, gregexpr("#\\w+", hashtags$tweets))
head(hashtags)
corpus = corpus(tweets, text_field = "tweets")
summary(corpus)
corpus = corpus(hashtags, text_field = "tweets")
hashtags$tweets <- regmatches(hashtags$tweets, gregexpr("#\\w+", hashtags$tweets))
hashtags$hashtag <- sapply(hashtags$tweets, paste, collapse=" ")
corpus = corpus(hashtags, text_field = "hashtag")
summary(corpus)
hashtags <- df[, c("username", "tweets")]
# this time we are interested only in the hashtags
hashtags$tweets <- regmatches(hashtags$tweets, gregexpr("#\\w+", hashtags$tweets))
hashtags$hashtag <- sapply(hashtags$tweets, paste, collapse=" ")
corpus = corpus(hashtags, text_field = "hashtag")
summary(corpus)
toks_hashtag = tokens(corpus) # tokenize the text (split each document into individual tokens)
toks_hashtag = tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE) # we remove punctuation and numbers from the token (they are noise)
dfmat2 = dfm(toks_hashtag) %>% dfm_trim(min_termfreq = 20)
dfmat2
hashtags$tweets <- regmatches(hashtags$tweets, gregexpr("#\\w+", hashtags$tweets))
hashtags$hashtag <- sapply(hashtags$tweets, paste, collapse=" ")
corpus = corpus(hashtags, text_field = "hashtag")
summary(corpus)
toks_hashtag = tokens(corpus) # tokenize the text (split each document into individual tokens)
toks_hashtag = tokens(doc.tokens, remove_punct = TRUE) # we remove punctuation and numbers from the token (they are noise)
dfmat2 = dfm(toks_hashtag) %>% dfm_trim(min_termfreq = 20)
dfmat2
features_dfm = textstat_frequency(dfmat, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
features_dfm = textstat_frequency(dfmat2, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
toks_hashtag = tokens(corpus) # tokenize the text (split each document into individual tokens)
toks_hashtag = tokens(toks_hashtag, remove_punct = TRUE) # we remove punctuation and numbers from the token (they are noise)
dfmat2 = dfm(toks_hashtag) %>% dfm_trim(min_termfreq = 20)
dfmat2
features_dfm = textstat_frequency(dfmat2, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
hashtags$hashtag <- sapply(hashtags$tweets, paste, collapse=" ")
head(hashtags$hashtag)
hashtags$hashtag
hashtags$hashtag_list <- regmatches(hashtags$tweets, gregexpr("#\\w+", hashtags$tweets))
hashtags$hashtag <- sapply(hashtags$tweets, paste, collapse=" ")
hashtags$hashtag
corpus = corpus(hashtags, text_field = "hashtag")
summary(corpus)
toks_hashtag = tokens(corpus) # tokenize the text (split each document into individual tokens)
toks_hashtag = tokens(toks_hashtag, remove_punct = TRUE) # we remove punctuation and numbers from the token (they are noise)
dfmat2 = dfm(toks_hashtag) %>% dfm_trim(min_termfreq = 20)
dfmat2
features_dfm = textstat_frequency(dfmat2, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
summary(corpus)
hashtags <- df[, c("username", "tweets")]
# this time we are interested only in the hashtags
hashtags$hashtag_list <- regmatches(hashtags$tweets, gregexpr("#\\w+", hashtags$tweets))
hashtags$hashtag <- sapply(hashtags$tweets, paste, collapse=" ")
hashtags$hashtag
corpus = corpus(hashtags, text_field = "hashtag")
summary(corpus)
toks_hashtag = tokens(corpus) # tokenize the text (split each document into individual tokens)
toks_hashtag = tokens(toks_hashtag, remove_punct = TRUE) # we remove punctuation and numbers from the token (they are noise)
dfmat2 = dfm(toks_hashtag) %>% dfm_trim(min_termfreq = 20)
dfmat2
features_dfm = textstat_frequency(dfmat2, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
hashtags$hashtag
head(hashtags$hashtag)
hashtags$hashtag_list <- regmatches(hashtags$tweets, gregexpr("#\\w+", hashtags$tweets))
hashtags$hashtag <- sapply(hashtags$hashtag_list, paste, collapse=" ")
head(hashtags$hashtag_list)
corpus = corpus(hashtags, text_field = "hashtag")
summary(corpus)
toks_hashtag = tokens(corpus) # tokenize the text (split each document into individual tokens)
toks_hashtag = tokens(toks_hashtag, remove_punct = TRUE) # we remove punctuation and numbers from the token (they are noise)
dfmat2 = dfm(toks_hashtag) %>% dfm_trim(min_termfreq = 20)
dfmat2
features_dfm = textstat_frequency(dfmat2, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
textplot_wordcloud(dfmat2)
textplot_wordcloud(dfmat)
library('syuzhet')  # use NRC Emotion Lexicon (list of words and their associations)
sentiment = get_nrc_sentiment(tweets$tweets)
textplot_wordcloud(dfmat2)
library(igraph)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
textplot_wordcloud(dfmat)
textplot_wordcloud(dfmat2)
library('syuzhet')
sentiment = get_nrc_sentiment(tweets$tweets)
tweets <- df[, c("username", "tweets")]
# we are interested only on the message in the tweet, not to link or mentions
tweets$tweets <- gsub("ENGLISH TRANSLATION:|rt", "", tweets$tweets, ignore.case = TRUE)
tweets$tweets <- gsub("http[^[:space:]]+", "", tweets$tweets)
tweets$tweets <- gsub("@\\w+", "", tweets$tweets)
tweets$tweets <- gsub("#\\w+", "", tweets$tweets)
sentiment = get_nrc_sentiment(tweets$tweets)
head(tweets)
dfmat
features_dfm
features_dfm = textstat_frequency(dfmat, n = 100)
features_dfm$feature = with(features_dfm, reorder(feature, -frequency))
features_dfm
library('syuzhet')
sentiment = get_nrc_sentiment(tweets$tweets)
sentiment = get_nrc_sentiment(tweets$tweets[1:100])
td = data.frame(t(sentiment))
td = data.frame(rowSums(td[-1]))
td
sentiment = get_nrc_sentiment(tweets$tweets)
td = data.frame(t(sentiment))
td = data.frame(rowSums(td[-1]))
td
td = data.frame(t(sentiment))
td = data.frame(rowSums(td[-1]))
names(td)[1] <- "count"
tdw <- cbind("sentiment" = rownames(td), td)
rownames(tdw) <- NULL
tdw
require("ggplot2")
# Plot Emotions
ggplot(td_em, aes(x = sentiment, y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(x = "emotion") +
theme(axis.text.x=element_text(angle=45, hjust=1), legend.title = element_blank())
require("ggplot2")
# Plot Emotions
ggplot(tdw[1:8, ], aes(x = sentiment, y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(x = "emotion") +
theme(axis.text.x=element_text(angle=45, hjust=1), legend.title = element_blank())
ggplot(tdw[9:10, ], aes(x = sentiment, y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(x = "polarity") +
theme(axis.text.x=element_text(angle=45, hjust=1), legend.title = element_blank())
# Plot Polarity
ggplot(tdw[9:10, ], aes(x = sentiment, y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(x = "polarity") +
theme(axis.text.x=element_text(angle=45, hjust=1), legend.title = element_blank())
head(mentions)
head(mention_counts)
net <- graph_from_data_frame(d=mention_counts, vertices = usernames, directed=T)
net <- simplify(net, remove.loops=T)
V(net)$size <- 5
l <- layout.fruchterman.reingold(net)
E(net)$width <- E(net)$weight/50
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
net <- graph_from_data_frame(d=mention_counts, vertices = usernames, directed=T)
net <- simplify(net, remove.loops=T)
V(net)$size <- 5
l <- layout.fruchterman.reingold(net)
E(net)$width <- E(net)$weight/50
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
deg <- degree(net, mode="in") # Node degree -> most mentioned nodes on the social
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
hist(deg, breaks=1:vcount(net)-1, main="Histogram of Node Degree")
deg.dist <- degree_distribution(net, cumulative=T, mode="in")
plot( x=0:max(deg), y=1-deg.dist, pch=19, cex=1.2, col="orange", xlab="Degree", ylab="Cumulative Frequency")
hist(deg, breaks=1:vcount(net)-1, main="Histogram of Node Degree")
deg <- degree(net, mode="out") # Node degree -> most active nodes on the social
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
hist(deg, breaks=1:vcount(net)-1, main="Histogram of Node Degree")
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
reciprocity(net)
dyad_census(net)
net <- net - V(net)[degree(net, mode="all")==0]
l <- layout.fruchterman.reingold(net)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_randomly, vertex.label=NA)
V(net)$size <- 10
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_randomly, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
V(net)$size <- 5
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_randomly, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
net <- graph_from_data_frame(d=mention_counts, vertices = usernames, directed=F)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
net <- graph_from_data_frame(d=mention_counts, vertices = usernames, directed=F)
net <- simplify(net, remove.loops=T)
V(net)$size <- 5
l <- layout.fruchterman.reingold(net)
E(net)$width <- E(net)$weight/50
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
net <- net - V(net)[degree(net, mode="all")==0]
l <- layout.fruchterman.reingold(net)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
net <- graph_from_data_frame(d=mention_counts, vertices = usernames, directed=F)
library(igraph)
net <- graph_from_data_frame(d=mention_counts, vertices = usernames, directed=F)
net <- simplify(net, remove.loops=T)
V(net)$size <- 5
l <- layout.fruchterman.reingold(net)
E(net)$width <- E(net)$weight/50
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
deg <- degree(net, mode="in") # Node degree -> most mentioned nodes on the social
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
hist(deg, breaks=1:vcount(net)-1, main="Histogram of Node Degree")
deg.dist <- degree_distribution(net, cumulative=T, mode="in")
plot( x=0:max(deg), y=1-deg.dist, pch=19, cex=1.2, col="orange", xlab="Degree", ylab="Cumulative Frequency")
deg <- degree(net, mode="out") # Node degree -> most active nodes on the social
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA, vertex.size=deg/3+3)
reciprocity(net)
dyad_census(net)
net <- net - V(net)[degree(net, mode="all")==0]
l <- layout.fruchterman.reingold(net)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA) # in this way we also get a cleaner graph
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_randomly, vertex.label=NA)
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=layout_in_circle, vertex.label=NA)
# since our network is basically a citation network, we can measure the centrality of the graph and see the most important vertex of the graph
# we will at first measure eigen centrality, that is a refinement that assigns higher weight to vertex for being connected to vertices which are themselves important
eigenCent <- eigen_centrality(net)$vector
# let's plot the score on the graph:
bins <- unique(quantile(eigenCent, seq(0,1,length.out=15)))
vals <- cut(eigenCent, bins, labels=FALSE, include.lowest=TRUE)
my_col = heat.colors(length(bins))
colorVals <- rev(my_col)[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# so who are the more relevant user?
sort(eigenCent,decreasing=TRUE)[1:10]
betweenCent <- betweenness(net)
cor(betweenCent,eigenCent)
bins <- unique(quantile(betweenCent, seq(0,1,length.out=30)))
vals <- cut(betweenCent, bins, labels=FALSE, include.lowest=TRUE)
colorVals <- rev(heat.colors(length(bins)))[vals]
V(net)$color <- colorVals
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
# Which are the most useful users to the graph connection?
sort(betweenCent,decreasing=TRUE)[1:10]
comm <- cluster_louvain(net)
plot(net, vertex.color=membership(comm), layout=l, vertex.label=NA, edge.arrow.size=0.1)
comm <- cluster_louvain(net)
plot(net, vertex.color=membership(comm), layout=l, vertex.label=NA, edge.arrow.size=0.1)
modularity(comm)
length(unique(membership(comm)))
table(membership(comm))
w <- edge.betweenness.community(net)
sort(table(w$membership))
w <- edge.betweenness.community(net)
sort(table(w$membership))
V(net)$color <- rep("white", length(w$membership))
keepTheseCommunities <- names(sizes(w))[sizes(w) > 3]
matchIndex <- match(w$membership, keepTheseCommunities) # like %in%
colorVals <- rainbow(5)[matchIndex[!is.na(matchIndex)]]
V(net)$color[!is.na(matchIndex)] <- colorVals
plot.igraph(net, vertex.label = NA, vertex.size=5)
plot.igraph(net, vertex.label = NA,layout=l, vertex.size=5)
V(net)$color <- rep("white", length(w$membership))
keepTheseCommunities <- names(sizes(w))[sizes(w) > 3]
matchIndex <- match(w$membership, keepTheseCommunities) # like %in%
colorVals <- rainbow(10)[matchIndex[!is.na(matchIndex)]]
V(net)$color[!is.na(matchIndex)] <- colorVals
plot.igraph(net, vertex.label = NA,layout=l, vertex.size=5)
V(net)$color <- rep("white", length(w$membership))
keepTheseCommunities <- names(sizes(w))[sizes(w) > 4]
matchIndex <- match(w$membership, keepTheseCommunities) # like %in%
colorVals <- rainbow(10)[matchIndex[!is.na(matchIndex)]]
V(net)$color[!is.na(matchIndex)] <- colorVals
plot.igraph(net, vertex.label = NA,layout=l, vertex.size=5)
