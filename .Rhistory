pairs <- matrix(0, nrow = length(months)-1, ncol=2)
for (m in 1:(length(months)-1)){
snap1 <- graph_list[[months[m]]]
snap2 <- graph_list[[months[m+1]]]
adj1 <- as_adj(snap1, sparse = FALSE)
adj2 <- as_adj(snap2, sparse=FALSE)
# first we get the common connections between each pair of nodes
com_connection <- adj1 %*% adj1 # this give us a matrix containing in each cell the number of path of length 2 between the nodes of row i and column j, so basically the number of common neighbors between node i and node j
# we now have to obtain only the pairs that haven't already formed an edge, so we are interested only in pair that aren't connected in adj1 and check if they have formed a connection in adj2, we use com_connection to know how many common connection they have so to save them into the right column
n <- nrow(adj1)
for(i in 1:(n-1)){
for(j in (i+1):n){
if(adj1[i,j]==0){  # if they don't have a connection in the first snaphshot
k <- com_connection[i,j]
if(k>1) # we group all the one with above 5 connection together
k <- 1
pairs[m,k+1] <- pairs[m,k+1] + 1
if(adj2[i,j]==1)
T_k[m, k+1] <- T_k[m, k+1] + 1 # increase the function
}
}
}
T_k[m,] <- T_k[m,] / pairs[m,]
}
T_k
pairs
T_k <- matrix(0, nrow = length(months)-1, ncol = 2) # when k is 5 and above in just one column
T_k_frac <- matrix(0, nrow = length(months)-1, ncol = 2)
pairs <- matrix(0, nrow = length(months)-1, ncol=2)
for (m in 1:(length(months)-1)){
snap1 <- graph_list[[months[m]]]
snap2 <- graph_list[[months[m+1]]]
adj1 <- as_adj(snap1, sparse = FALSE)
adj2 <- as_adj(snap2, sparse=FALSE)
# first we get the common connections between each pair of nodes
com_connection <- adj1 %*% adj1 # this give us a matrix containing in each cell the number of path of length 2 between the nodes of row i and column j, so basically the number of common neighbors between node i and node j
# we now have to obtain only the pairs that haven't already formed an edge, so we are interested only in pair that aren't connected in adj1 and check if they have formed a connection in adj2, we use com_connection to know how many common connection they have so to save them into the right column
n <- nrow(adj1)
for(i in 1:(n-1)){
for(j in (i+1):n){
if(adj1[i,j]==0){  # if they don't have a connection in the first snaphshot
k <- com_connection[i,j]
if(k>1) # we group all the one with above 5 connection together
k <- 1
pairs[m,k+1] <- pairs[m,k+1] + 1
if(adj2[i,j]==1)
T_k[m, k+1] <- T_k[m, k+1] + 1 # increase the function
}
}
}
T_k_frac[m,] <- T_k[m,] / pairs[m,]
}
T_k
pairs
T_k
pairs
T_k_tot <- colSums(T_k)
pairs_tot <- colSums(pairs)
T_k_tot
pairs_tot
T_k_tot <- T_k_tot / pairs_tot
T_k_tot
barplot(T_k_tot)
barplot(T_k_tot, beside=T, names.arg=c("0", "≥1"))
barplot(T_k_tot, beside=T, names.arg=c("0", "≥1"), legend.text = c("No common neighbor", "At least 1 common neighbor"),)
barplot(T_k_tot, beside=T, names.arg=c("0", "≥1"), legend.text = c("No common neighbor", "At least 1 common neighbor"),  args.legend = list(x = "topright"),)
barplot(T_k_tot, beside=T, names.arg=c("0", "≥1"), legend.text = c("No common neighbor", "At least 1 common neighbor"),  args.legend = list(x = "bottomright"),)
barplot(T_k_tot, beside=T, names.arg=c("0", "≥1"),main = "T(k) per month",
ylab = "Number of new links")
barplot(T_k_tot, beside=T, names.arg=c("0", "≥1"),main = "T(k) per month",
ylab = "Probability of nodes connection")
barplot(T_k_tot, beside=T, names.arg=c("0 common neighbors", "≥1 common neighbors"),main = "T(k) per month",
ylab = "Probability of nodes connection")
barplot(T_k, names.arg=months[-1])
barplot(t(T_k), names.arg=months[-1])
barplot(t(T_k_frac), names.arg=months[-1])
barplot(t(T_k), names.arg=months[-1])
barplot(t(T_k), names.arg=months[-1], col=c("red", "blue"),   legend.text = c("0 common neighbor", "≥1 common neighbor"))
barplot(T_k_tot, beside=T, names.arg=c("0 common neighbors", "≥1 common neighbors"), main = "T(k) per month",
ylab = "Probability of nodes connection")
barplot(T_k_tot, beside=T, names.arg=c("0 common neighbors", "≥1 common neighbors"), main = "T(k) per month",
ylab = "Probability of nodes connection", col=c("red", "blue"))
w <- cluster_edge_betweenness(net2)
sort(table(w$membership)) # vector that associate each node with a community that represent the nodes that interact with each other mostly.
# we have 5 significant communities with 9 or more nodes, let's remake the graph with different color for each community -> graphical visualization:
V(net2)$color <- rep("white", length(w$membership))
keepTheseCommunities <- names(sizes(w))[sizes(w) > 4]
matchIndex <- match(w$membership, keepTheseCommunities) # like %in%
colorVals <- rainbow(10)[matchIndex[!is.na(matchIndex)]]
V(net2)$color[!is.na(matchIndex)] <- colorVals
plot.igraph(net2, vertex.label = NA,layout=layout, vertex.size=5)
communities <- cluster_lovain(net2)
communities <- cluster_louvain(net2)
communities
V(net2)$community <- membership(communities)
V(net2)$community
centrality_df <- data.frame(
node = V(net2)$username,
community = V(net2)$community,
degree = degree(net2),
betweenness = betweenness(net2),
closeness = closeness(net2)
)
centrality_df <- data.frame(
node = V(net2)$name,
community = V(net2)$community,
degree = degree(net2),
betweenness = betweenness(net2),
closeness = closeness(net2)
)
head(centrality_df)
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
betweenness = betweenness(net2),
closeness = closeness(net2)
)
head(centrality_df)
centrality_df
aggregate(. ~ community, data = centrality_df[, -1], FUN = mean)
aggregate(. ~ community, data = centrality_df, FUN = mean)
communities
communities <- fastgreedy.community(net2)
communities
communities <- multilevel.community(g_likes)
communities
communities <- multilevel.community(net2)
communities
V(net2)$community <- membership(communities)
V(net2)$community
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
betweenness = betweenness(net2), # how much a node control the flow of information
closeness = closeness(net2)      # how fast a node can reach other nodes, the inverse of distances
)
aggregate(. ~ community, data = centrality_df, FUN = mean) # we check how the communities behave in the newtork
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
centrality = eigen_centrality(net2),
betweenness = betweenness(net2), # how much a node control the flow of information
closeness = closeness(net2)      # how fast a node can reach other nodes, the inverse of distances
)
aggregate(. ~ community, data = centrality_df, FUN = mean)
warnings()
centrality_df
eigen_centrality(net2)
eigen_centrality(net2)$vector
betweenness(net2)
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
centrality = eigen_centrality(net2)$vector,
betweenness = betweenness(net2), # how much a node control the flow of information
closeness = closeness(net2)      # how fast a node can reach other nodes, the inverse of distances
)
aggregate(. ~ community, data = centrality_df, FUN = mean) # we check how the communities behave in the newtork
E(net2)$weight
communities <- multilevel.community(net2, weights = E(net2)$weight)
communities
V(net2)$community <- membership(communities)
V(net2)$community
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net)$weight),
centrality = eigen_centrality(net2)$vector,  # how central a community is
betweenness = betweenness(net2), # how much a node control the flow of information
closeness = closeness(net2)      # how fast a node can reach other nodes, the inverse of distances
)
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2)$vector,  # how central a community is
betweenness = betweenness(net2), # how much a node control the flow of information
closeness = closeness(net2)      # how fast a node can reach other nodes, the inverse of distances
)
aggregate(. ~ community, data = centrality_df, FUN = mean)
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2)$vector,  # how central a community is
betweenness = betweenness(net2, weights = inv_weight), # how much a node control the flow of information
closeness = closeness(net2, weights = inv_weight)      # how fast a node can reach other nodes, the inverse of distances
)
inv_weights <- 1 / E(net2)$weight
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2)$vector,  # how central a community is
betweenness = betweenness(net2, weights = inv_weights), # how much a node control the flow of information
closeness = closeness(net2, weights = inv_weights)      # how fast a node can reach other nodes, the inverse of distances
)
aggregate(. ~ community, data = centrality_df, FUN = mean) # we check how the communities behave in the network
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2, weights = E(net2)$weight)$vector,  # how central a community is
betweenness = betweenness(net2, weights = inv_weights), # how much a node control the flow of information
closeness = closeness(net2, weights = inv_weights)      # how fast a node can reach other nodes, the inverse of distances
)
aggregate(. ~ community, data = centrality_df, FUN = mean) # we check how the communities behave in the network
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2, weights = inv_weights)$vector,  # how central a community is
betweenness = betweenness(net2, weights = inv_weights), # how much a node control the flow of information
closeness = closeness(net2, weights = inv_weights)      # how fast a node can reach other nodes, the inverse of distances
)
aggregate(. ~ community, data = centrality_df, FUN = mean) # we check how the communities behave in the network
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2, weights = E(net2)$weight)$vector,  # how central a community is
betweenness = betweenness(net2, weights = inv_weights), # how much a node control the flow of information
closeness = closeness(net2, weights = inv_weights)      # how fast a node can reach other nodes, the inverse of distances
)
aggregate(. ~ community, data = centrality_df, FUN = mean) # we check how the communities behave in the network
colorVals <- c("red", "blue", "yellow", "green", "violet", "darkgreen", "orange", "skyblue")
V(net2)$color <- colorVals[community]
colorVals <- c("red", "blue", "yellow", "green", "violet", "darkgreen", "orange", "skyblue")
centrality_df <- data.frame(
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2, weights = E(net2)$weight)$vector,  # how central a community is
betweenness = betweenness(net2, weights = inv_weights), # how much a node control the flow of information
closeness = closeness(net2, weights = inv_weights),    # how fast a node can reach other nodes, the inverse of distances
color = colorVals[V(net2)$community]
)
centrality_df
aggregate(. ~ community, data = centrality_df, FUN = mean) # we check how the communities behave in the network
colorVals <- c("red", "blue", "yellow", "green", "violet", "darkgreen", "orange", "skyblue")
V(net2)$color <- colorVals[V(net2)$community]
plot.igraph(net2, vertex.label = NA,layout=layout, vertex.size=5)
library('syuzhet')
sentiment = get_nrc_sentiment(comtweets[comtweets$community==1,]$tweets)
names(td)[1] <- "count"
tdw <- cbind("sentiment" = rownames(td), td)
rownames(tdw) <- NULL
tdw
ggplot(tdw[1:8, ], aes(x = sentiment, y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(x = "emotion") +
theme(axis.text.x=element_text(angle=45, hjust=1), legend.title = element_blank())
require("ggplot2")
# Plot Emotions
ggplot(tdw[1:8, ], aes(x = sentiment, y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(x = "emotion") +
theme(axis.text.x=element_text(angle=45, hjust=1), legend.title = element_blank())
top_users
top_users <- centrality_df %>%
filter(community %in% c(1, 2, 3)) %>%
group_by(community) %>%
top_n(3, centrality) %>%
arrange(community, desc(centrality))
library(igraph)
library(lubridate)
library(dplyr)
top_users <- centrality_df %>%
filter(community %in% c(1, 2, 3)) %>%
group_by(community) %>%
top_n(3, centrality) %>%
arrange(community, desc(centrality))
top_users
centrality_df <- data.frame(
username = V(net2)$name,
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2, weights = E(net2)$weight)$vector,  # how central a community is
betweenness = betweenness(net2, weights = inv_weights), # how much a node control the flow of information
closeness = closeness(net2, weights = inv_weights)    # how fast a node can reach other nodes, the inverse of distances
)
centrality_df
top_users <- centrality_df %>%
filter(community %in% c(1, 2, 3)) %>%
group_by(community) %>%
top_n(3, centrality) %>%
arrange(community, desc(centrality))
top_users
sentiment = get_nrc_sentiment(comtweets[comtweets$username=="mobi_ayubi",]$tweets)
sentiment = get_nrc_sentiment(comtweets[comtweets$community==1,]$tweets)
communities <- multilevel.community(net2, weights = E(net2)$weight) # create community based on the strength of ties, in this way we group strong mutual connections more tightly
communities
V(net2)$community <- membership(communities)
V(net2)$community
inv_weights <- 1 / E(net2)$weight
centrality_df <- data.frame(
username = V(net2)$name,
community = V(net2)$community,
degree = degree(net2),
strength = strength(net2, weights = E(net2)$weight),
centrality = eigen_centrality(net2, weights = E(net2)$weight)$vector,  # how central a community is
betweenness = betweenness(net2, weights = inv_weights), # how much a node control the flow of information
closeness = closeness(net2, weights = inv_weights)    # how fast a node can reach other nodes, the inverse of distances
)
centrality_df
colorVals <- c("firebrick3", "slateblue1", "yellow1", "olivedrab1", "pink", "seagreen1", "orange", "turquoise1")
V(net2)$color <- colorVals[V(net2)$community]
plot.igraph(net2, vertex.label = NA,layout=layout, vertex.size=5)
aggregate(. ~ community, data = centrality_df[-1], FUN = mean)
# we can see that the most important communities under every aspect are community 1, 2 and 3
# let's now see who are the users that are more important in each of these community:
top_users <- centrality_df %>%
filter(community %in% c(1, 2, 3)) %>%
group_by(community) %>%
top_n(3, centrality) %>%
arrange(community, desc(centrality))
top_users
head(tweets)
# we already have a data frame containing the messagges and the users, we need to keep only the users that are in a community and specify the community of each user
comtweets <- tweets %>% inner_join(centrality_df %>% select(username, community))
head(comtweets)
sentiment = get_nrc_sentiment(comtweets[comtweets$community==1,]$tweets)
td = data.frame(t(sentiment))
td = data.frame(rowSums(td[-1]))
names(td)[1] <- "count"
tdw <- cbind("sentiment" = rownames(td), td)
rownames(tdw) <- NULL
tdw
require("ggplot2")
# Plot Emotions
ggplot(tdw[1:8, ], aes(x = sentiment, y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(x = "emotion") +
theme(axis.text.x=element_text(angle=45, hjust=1), legend.title = element_blank())
library('syuzhet')  # use NRC Emotion Lexicon (list of words and their associations)
sentiment = get_nrc_sentiment(comtweets[comtweets$username=="mobi_ayubi",]$tweets)
td = data.frame(t(sentiment))
td = data.frame(rowSums(td[-1]))
names(td)[1] <- "count"
tdw <- cbind("sentiment" = rownames(td), td)
rownames(tdw) <- NULL
tdw
require("ggplot2")
# Plot Emotions
ggplot(tdw[1:8, ], aes(x = sentiment, y = count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(x = "emotion") +
theme(axis.text.x=element_text(angle=45, hjust=1), legend.title = element_blank())
community_id <- unique(V(net2)$community)
density <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(net2, loops=F))
}
density
community_id <- unique(V(net2)$community)
density <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(sub_net, loops=F))
}
density
edge_density(net2, loops=F)
density <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(sub_net, loops=F))
}
density
density <- c()
nnodes <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(sub_net, loops=F))
nnodes <- c(nnodes, vcount(sub_net))
}
density # we can see that the density inside each community is significantly higher than in the general network, we can see that the communities 1-4 have a lower density but it's normal since the have more nodes
nnodes
density <- c()
nnodes <- c()
distance <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(sub_net, loops=F))
nnodes <- c(nnodes, vcount(sub_net))
diameter <- c(diameter, diameter(sub_net))
}
density # we can see that the density inside each community is significantly higher than in the general network, we can see that the communities 1-4 have a lower density but it's normal since the have more nodes
nnodes # indeed having 20 nodes and a density of 0.15 means that the network is sparse but not a lot, indeed the distance of each community is:
diameter
diameter <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(sub_net, loops=F))
nnodes <- c(nnodes, vcount(sub_net))
diameter <- c(diameter, diameter(sub_net))
}
density # we can see that the density inside each community is significantly higher than in the general network, we can see that the communities 1-4 have a lower density but it's normal since the have more nodes
nnodes # indeed having 20 nodes and a density of 0.15 means that the network is sparse but not a lot, indeed the distance of each community is:
diameter
density # we can see that the density inside each community is significantly higher than in the general network, we can see that the communities 1-4 have a lower density but it's normal since the have more nodes
nnodes # indeed having 20 nodes and a density of 0.15 means that the network is sparse but not a lot, indeed the distance of each community is:
diameter
density
community_id <- unique(V(net2)$community)
edge_density(net2, loops=F)
density <- c()
nnodes <- c()
diameter <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(sub_net, loops=F))
nnodes <- c(nnodes, vcount(sub_net))
diameter <- c(diameter, diameter(sub_net))
}
density # we can see that the density inside each community is significantly higher than in the general network, we can see that the communities 1-4 have a lower density but it's normal since the have more nodes
nnodes # indeed having 20 nodes and a density of 0.15 means that the network is sparse but not a lot, indeed the distance of each community is:
diameter
density <- c()
nnodes <- c()
diameter <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
if (is.directed(sub_net)) {
sub_net <- as.undirected(sub_net)
}
density <- c(density, edge_density(sub_net, loops=F))
nnodes <- c(nnodes, vcount(sub_net))
diameter <- c(diameter, diameter(sub_net))
}
density # we can see that the density inside each community is significantly higher than in the general network, we can see that the communities 1-4 have a lower density but it's normal since the have more nodes
nnodes # indeed having 20 nodes and a density of 0.15 means that the network is sparse but not a lot, indeed the distance of each community is:
diameter
sub_net <- induced.subgraph(net2, V(net2)[community==1])
diameter(sub_net)
vcount(sub_net)
plot(sub_net)
plot(sub_net, vertex.label=NA)
get_diameter(sub_net)
nrow(get_diameter(sub_net))
length(get_diameter(sub_net))
density <- c()
nnodes <- c()
diameter <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(sub_net, loops=F))
nnodes <- c(nnodes, vcount(sub_net))
diameter <- c(diameter, length(get_diameter(sub_net)))
}
density # we can see that the density inside each community is significantly higher than in the general network, we can see that the communities 1-4 have a lower density but it's normal since the have more nodes
nnodes # indeed having 20 nodes and a density of 0.15 means that the network is sparse but not a lot, indeed the distance of each community is
diameter
density <- c()
nnodes <- c()
diameter <- c()
transitivity <- c()
for (i in community_id){
sub_net <- induced.subgraph(net2, V(net2)[community==i])
density <- c(density, edge_density(sub_net, loops=F))
nnodes <- c(nnodes, vcount(sub_net))
diameter <- c(diameter, length(get_diameter(sub_net)))
transitivity <- c(transitivity, transitivity(sub_net))
}
density # we can see that the density inside each community is significantly higher than in the general network, we can see that the communities 1-4 have a lower density but it's normal since the have more nodes
nnodes # indeed having 20 nodes and a density of 0.15 means that the network is sparse but not a lot, indeed the diameter of the communities is around 5-6, we can say that the communities are sparse but definitely less than the network
diameter
transitivity
assortativity_nominal(net2, types = as.factor(V(net2)$community), directed=F)
library(igraph)
library(lubridate)
library(dplyr)
library('syuzhet')
require("ggplot2")
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
df <- read.csv("tweets.csv") # import the data
head(df)
usernames <- unique(df$username)
usernames <- unique(df$username)
results <- list()
row_index <- 1
# get a list of pair: user that mention, user mentioned
for (i in 1:nrow(df)) {
from_user <- df$username[i]
tweet_text <- df$tweets[i]
mentions <- regmatches(tweet_text, gregexpr("@\\w+", tweet_text))[[1]] # get the string in the text that are composed by @*username*
if (length(mentions) > 0) {
for (mention in mentions) {
to_user <- sub("@", "", mention)  # remove the @ from the username
if(to_user %in% usernames)  #  save only if the mentioned user is in our data set
results[[row_index]] <- data.frame(from = from_user, to = to_user, stringsAsFactors = FALSE)
row_index <- row_index + 1
}
}
}
mentions <- do.call(rbind, results) # transform the list of dataframe in a dataframe
head(mentions)
users <- unique(c(mentions$from, mentions$to)) # list of users mentioned in the edges (we need it later)
#
mention_counts <- as.data.frame(table(mentions$from, mentions$to), stringsAsFactors = FALSE)
colnames(mention_counts) <- c("from", "to", "weight")
mention_counts <- mention_counts[mention_counts$weight > 0, ]
head(mention_counts)
net <- graph_from_data_frame(d=mention_counts, vertices = usernames, directed=T)
net <- simplify(net, remove.loops=T)
V(net)$size <- 5
l <- layout.fruchterman.reingold(net)
E(net)$width <- E(net)$weight/50
plot(net, edge.arrow.size=.1, edge.curved=.1, layout=l, vertex.label=NA)
